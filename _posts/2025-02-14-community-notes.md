---
layout: post
title: "Community Notes"
date: 2025-02-14
---

Community Notes is a community-driven content moderation system designed to combat misinformation and provide context. It was launched by Twitter in 2021 and later expanded by X. Although still in its infancy, I consider it a very meaningful feature because it directly attacks algorithmic echo chambers.

To join the Community Notes program, you must have an X account that is at least six months old, a verified phone number, and no recent violations of the platform’s rules. New users can only rate notes, they can’t propose them. Contributors can’t see each other’s X profiles to reduce bias. Proposed notes appear in a dedicated view, listed with their current status and buttons to rate them as “Helpful,” “Somewhat helpful,” or “Not helpful.” When you vote, a list of reasons appears, such as the quality of the sources or the neutrality of the tone, and you can select any that apply before confirming your vote.

The key aspect of the system is that users with different voting histories must agree that a note is helpful for it to be displayed under a post. If a note doesn’t receive enough votes from ideologically diverse users, it remains unpublished.

Anyone can get community-noted. It’s happened to the President of the United States, the White House, the FBI, and platform owner Elon Musk who has been corrected many times. Notes can even be proposed for ads. I can’t recall seeing a published note on a regular post that I found unhelpful.

In 2023, X began to demonetize posts that receive a note, preventing users from earning ad dollars with BS. That same year, X removed the option to report posts for misinformation – except in the EU, where it’s legally required – shifting the responsibility to Community Notes. Those in favor of broader internet censorship disapprove of these changes.

While illegal content must always be taken down, keeping posts that are misleading or false online – with added context – seems to be the most reasonable approach, even if it’s painful to watch what some people put out there. It’s important to understand that at this scale, perfection is unattainable. Whether you remove content or leave it up, every approach comes with its own collateral damage. Removing content for misinformation – usually done by automated systems prone to errors – can hit legitimate posts, while leaving it up – even with added context – can allow misleading or false content to spread.

Another problem is that “misleading” doesn’t mean the same thing to everyone, putting aside obvious claims like the sky is red. People can’t even agree on what qualifies as humor: some say a post is funny, while others say it’s misleading. I think it’s generally healthier to attach context to a post rather than pretend it never existed. It also gives a stronger incentive not to post BS when everyone can see the context provided by the community.

One problem with Community Notes is its slow pace due to the system’s voting mechanism. Ideally, a post should get a note before it spikes in views. Over time, as more contributors join, the rating process should speed up, but it might still not be fast enough. One potential solution is to use generative AI, although this must be implemented very carefully. If the system detects a problematic claim, it could generate an initial note while still letting people vote on it. This would also address an issue I often notice in proposed notes – people don’t use a neutral tone or they overcomplicate things. If a fine-tuned language model generates the note, fewer votes might be needed for it to be published. For posts that the system is confident contain blatant falsehoods, it could display an AI-generated note immediately, clearly marked as such. This would attach notes to posts as soon as they get posted, giving an even stronger incentive not to post BS.